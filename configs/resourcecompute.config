import java.io.File
import groovy.json.JsonSlurper

System.out.println("Info: Please hold!\nCalculating computing resources needed based on the number of samples and the size of the reference genome")

def command2 = ["sh","-c","echo sleeping for 5 second; sleep 5s"]
def sleeprocess2 = command2.execute()
sleeprocess2.waitFor()

if(!new File("${params.refgenome}"))
{
   System.err.println("Error: The path to the reference genome '"+params.refgenome+"' is not valid")
   System.exit(1)
}

//Options to obtain the sample IDs from samplesheet Or GATKDB files
def fileName = params.samplesheet
def filetype = "csv"
System.out.println("Samplesheet being used: "+fileName)
def skipalignhapdb=false
// Logic for conditional file selection
if (!params.GATKupdateexistingdb && new File(params.GATKpathtodbs).exists() && (new File(params.samplesheet == null))) {
    System.err.println("Info: Getting sample information from GATKDB files")
    filetype = "json"
    skipalignhapdb = true  // Ensure this variable is defined outside the method
    def callsetJsonFile = new File(params.GATKpathtodbs).listFiles().findResult {
        it.isDirectory() && new File(it, "callset.json").exists() ? new File(it, "callset.json") : null
    }
    if (callsetJsonFile == null) {
        System.err.println("Error: callset.json file not found in GATKDBs directory")
        System.exit(1)
    }
    fileName = callsetJsonFile.toString()
}

def sampleIDs = []


switch (filetype) {
    case "csv":
        // Groovy's File.eachLine simplifies CSV parsing
        new File(fileName).eachLine { line, lineNumber ->
            def row = line.split(",")
            def read1 = row.size() > 2 ? row[row.size() - 3] : row[1]
            if (new File(read1).exists()) {
                sampleIDs.add(row[0])
            }
        }
        break

    case "json":
        // Directly read sample IDs from the JSON
        try {
            def jsonData = new JsonSlurper().parse(new File(fileName))
            sampleIDs = jsonData.callsets.sample_name
        } catch (Exception e) {
            System.err.println("Error parsing JSON file: ${e.message}")
            System.exit(1)
        }
        break

    default:
        System.err.println("Invalid file type: $type")
        System.exit(1)
}
def refgenome=new File("${params.refgenome}").getName()

def genomesizefile = new File("${projectDir}/genomsizes/")

def genomesize=3000000000L  // Use long for genome size

def maxchromsize=0L

if(genomesizefile.canWrite())
  genomesizefile=new File("${projectDir}/genomsizes/"+refgenome+".txt")
else
   genomesizefile=new File(refgenome+".txt")


// Simplified genome size reading and writing
if (genomesizefile.exists()) {
    genomesizefile.withReader { reader ->
        def line = reader.readLine()
        genomesize = line.split(";")[0].toLong()
        maxchromsize = line.split(";")[2].toLong()
    }
} else {
    //Build dictionary file
    File fastafile = new File(params.refgenome)
    def path2fasta = fastafile.getParent()
    def dict = fastafile.baseName + ".dict"

    if (new File(path2fasta).canWrite()) {
        dict = path2fasta + "/" + dict
    }
    def picardcommand = ["sh", "-c", "picard CreateSequenceDictionary R=${params.refgenome} O=${dict}"]

    // Execute the command if the dictionary file doesn't exist
    if (!new File(dict).exists()) {
        System.out.println("Building dictionary for " + params.refgenome)
        def process = picardcommand.execute()
        System.out.println(process.toString())
        process.waitFor() // Wait for the command to finish

        // Check for errors
        if (process.exitValue() != 0) {
            System.out.println("Error creating FASTA dictionary. Here's the command output:")
            System.out.println(process.err.text)
            return null // Or handle the error in another way
        } else {
            System.out.println("FASTA dictionary created successfully!")
        }
    }

    // Open the FASTA file for reading
    BufferedReader reader = new BufferedReader(new FileReader(dict))
    // Initialize the genome size.
    long genomeSize = 0
    def chromSizes = []

    // Read the dictionary file line by line.
    String line
    while ((line = reader.readLine()) != null) {
        // Ignore the header line
        if (line.startsWith("@HD")) {
            continue
        }
        List<String> row = line.split("\t")
        long chrosize = Long.parseLong(row[2].replaceAll("LN:", ""))
        chromSizes.add(chrosize)
        genomeSize += chrosize
    }

    // Close the file.
    reader.close()

    // Return the genome size, min chromosome size, and max chromosome size.
    def result=[genomeSize, chromSizes.min(), chromSizes.max()]
    System.out.println(result)  // Print result for debugging
    genomesizefile.withWriter { writer ->
        writer.println(result.join(";"))
    }
    genomesize = result[0]
    maxchromsize = result[2]
}

System.out.println("")
//Check if to skip Haplotype calling and building of database
params.skipalignhapdb=skipalignhapdb
def largegenome=10000000000 //Define large genome
def numberofsamples=sampleIDs.size()
def numberofIDs=sampleIDs.unique().size()

if(numberofIDs==0)
{
  System.err.println("Error: Number of verified paths to read1 can't be 0, check samplesheet to ensure correct/absolute paths have been provided")
  System.exit(1)
}
if(params.mode=="test")
{
  numberofIDs=2
}


params.numberofsamples=numberofIDs
def sampsperid= numberofIDs / numberofsamples

//Determine resources for Alignment step
def alignlabel="verysmall"
if(genomesize >= largegenome*2)
    alignlabel="small"

if(genomesize >= largegenome*2.5)
    alignlabel="medium"


params.alignlabel=alignlabel

//Determine resources for merge step
def mergelabel="medium"
if(genomesize >= largegenome && sampsperid >=4)
    mergelabel="verylarge"
else if(genomesize >= largegenome )
    mergelabel="large"

params.mergelabel=mergelabel

//Parameter for GATHER VCF step
def gathervcfs="medium"

if(numberofIDs>500&&numberofIDs<=1000)
   gathervcfs="large"
else if(numberofIDs>1000)
   gathervcfs="verlarge"

params.gathervcfs=gathervcfs

//Determine resources for fastqc

def fastqclabel="small"
if(numberofsamples >= 100 && numberofsamples < 500 )
    fastqclabel="medium"
else if(numberofsamples >= 500 && numberofsamples < 1000 )
    fastqclabel="large"
else if (numberofsamples >= 1000 )
    fastqclabel="verylarge"
params.fastqclabel=fastqclabel

//Determine resources for GATK processes not multi-threaded
def gatk_hap="GATKsmall"

//determine resources for multi-threaded GATK processes
def gatkmulti="GATKmedium"
if(( numberofIDs >= 200 && numberofIDs < 1000 ) || genomesize >= largegenome )
{
    gatkmulti="GATKlarge"
    //gatk_hap="GATK_medium"
}
else if (numberofIDs >= 1000 )
{
  gatkmulti="GATKverylarge"
  gatk_hap="haplotyping"
}

params.gatkhap=gatk_hap
params.gatkmulti=gatkmulti

def subintervals=false

//Check if there is need to split intervals further during joint genotyping step
if(numberofIDs>500 && genomesize >= largegenome )
{
  subintervals=true
}
params.subintervals=subintervals

//Check if the size of interval is not greater than genome size
if(params.intervalsizegatk > maxchromsize) {
    System.err.println("Error: Interval size of ${params.intervalsizegatk} exceeds the maximum chromosome size of ${maxchromsize}")
    System.exit(1)
}
def command = ["sh","-c","echo sleeping for 5seconds; sleep 5s"]
def process = command.execute()
process.waitFor()
System.out.println("Info: Calculation of computing resources complete.") // Concise message